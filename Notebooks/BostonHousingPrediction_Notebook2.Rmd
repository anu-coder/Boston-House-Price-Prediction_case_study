---
title: "Boston House Price Prediction"
subtitle: 'Notebook 2, continuation of Notebook 1. Using Random forest, Grid search.'
author: 
  name: "Anurima Dey"
date: 28-April-2020
output: 
  html_document:
  df_print: paged
---

## Purpose of the project: 

This notebook contains the non-regression models for the price prediction of Boston Housing.
Do refer to notebook 1, for all the steps, and visualization of the data and corelation among various attibutes. We shall keep the basic steps same i.e. obtaining the data and the cleaning of data remains same. 

Following that we shall proceed directly with the non regression models.

### Obtaining and cleaning the data. 


The data is obtained from kaggle website. <br> The displayed are the __features__ of the dataset.  


1. *CRIM*     – per capita crime rate by town <br>
2. *ZN*       – proportion of residential land zoned for lots over 25,000 sq.ft <br>
3. *INDUS*    – proportion of non-retail business acres per town <br>
4. *CHAS*     – Charles River dummy variable (1 if tract bounds river; else 0) <br>
5. *NOX*      – nitric oxides concentration (parts per 10 million) <br>
6. *RM*       – average number of rooms per dwelling <br>
7. *AGE*      – proportion of owner-occupied units built prior to 1940 <br>
8. *DIS*      – weighted distances to five Boston employment centres<br>
9. *RAD*      – index of accessibility to radial highways <br>
10. *TAX*      – full-value property-tax rate per $10,000 <br>
11. *PTRATIO*  – pupil-teacher ratio by town <br>
12. *B*        – 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town <br>
13. *LSTAT*    – % lower status of the population <br>
14. *MEDV*     – Median value of owner-occupied homes in $1000’s

Read the dataset as a dataframe using read.table. After loading and cleaning the data set looks like the following.

```{r}
data.file <- normalizePath("../Data/housing.csv")
Bhousing <- read.table(data.file, header = FALSE, sep= "")
column_names=c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 
                'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV')
names(Bhousing)=column_names
head(Bhousing)

```


```{r}
class(Bhousing)
```
Now, let us check and explore the cleaned data frame containing the housing data. The __Bhousing__ R object is of class ‘data.frame’. <br> The __str()__ function displays the structure of an R dataframe.

#### What does __str()__ show:
1. The data type.
2. Name and data types of each column.
3. Some of the observations.

```{r}
str(Bhousing)
```

To get more detailed statistical information from each column, __summary()__ function can be used. It displays the summary statistics like *minimum value, maximum value, median, mean, and the 1st and 3rd quartile* values for each column in our dataset.<br> Summary also provides information about the missing values, if any is present. We see that there are no missing values in any of our variables.

```{r}
summary(Bhousing)
```

Till this part it remains same as our __BostonHousingPrediction_notebook1__. Next we create the training and testing dataset. The training dataset will contain 70% of the data and the testing dataset will contain remaining 30% of the data. To do so here we use a library caret, and create data partition using inbuilt functions like createDataPartition. We could otherwise have divided the data frame manually in to first 70% and remaining 30%.   

```{r}
library(caret)
Bhousing.scale=cbind(scale(Bhousing[1:13]), Bhousing[14])
set.seed(1)

inTrain <- createDataPartition(y=Bhousing.scale$MEDV, p=0.70, list= F)
Bhousing.training <- Bhousing.scale[inTrain,]
Bhousing.testing <- Bhousing.scale[-inTrain,]
head(Bhousing.training)

```

### RANDOM FOREST 

Next we shall be using the inbuilt library CARET to prdict the outcome using random forest. Along with that we shall be using cross validation tuning method to find the actual efficiency of the model.

```{r}
library(caret)
library(randomForest)
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random')

#Random generate 15 mtry values with tuneLength = 15

set.seed(1)
rf_random <- train(log(MEDV) ~ .,
                   data = Bhousing.training,
                   method = 'rf',
                   tuneLength  = 15, 
                   trControl = control)
print(rf_random)

```

The above prediction shows the best model is the one with the parameter __mtry=5__
The below graph shows how the error decreases with the increase of the __ntree__ parameter and reaches a minimum at 500. 

```{r}
plot(rf_random$finalModel)
```

This is a plot for mtry optimization carried out by the data processing.

```{r}
plot(rf_random)
```

Now, we shall do the prediction using the inbuilt prediction function under the caret package.

```{r}
set.seed(1)
pred.m2=predict(rf_random, newdata=Bhousing.testing)
actuals_predicts= data.frame(cbind(actuals=Bhousing.testing$MEDV, predicts=exp(pred.m2)))

head(actuals_predicts)

```

We see that after prediction the $RMSE= \sqrt{\frac{\sum(y_{pred}-y_{act})}{n}}$ value is lesser than then all the other models. 

All the errors that one can have are: </br>
__RMSE__:  Root Mean Squared Error </br>
__MPE__:   Mean Percentage error </br>
__MAPE__:  Mean Absolute Percentage Error </br>
__MSE__:   Mean Squared Error </br>
__MAE__:   Mean Absolute Error </br>

```{r}
library(DMwR)
library(lattice)
library(grid)
set.seed(1)
r=regr.eval(actuals_predicts$actuals,actuals_predicts$predicts)
r
```

The percentage of RMSE is: 

```{r}
r_1=as.data.frame(r)
r_1[3,]/mean(Bhousing$MEDV)*100 
```

Now we shall try to see how good is the prediction graphically.

We have a predefined function for this, available in my github repo, __anu-coder/R_utils__, I will be intalling the same to find out this plot. One can use the help function to find out the details of the function being used here after installing the package.

```{r}
library(devtools)
library(usethis)
devtools::install_github("anu-coder/R_utils")
library(testcase)
compare.dens(actuals_predicts$predicts, actuals_predicts$actuals, legend = c("Predicts","Observed"))
```

The percentage of RMSE and also this curve fitting, shows that the model is pretty error free while using Random Forest with repeated Cross Validation. 

### GRID Search

Next we shall be using *grid search* technique and see if the model improves at all. 

```{r}
library(caret)
library(randomForest)
control <- trainControl(method='cv', 
                        number=10, 
                        search = 'grid')

#Random generate 15 mtry values with tuneLength = 15

tuneGrid <- expand.grid(mtry=c(1:13))
set.seed(10)
rf_random_grid <- train(log(MEDV) ~ .,
                   data = Bhousing.training,
                   method = 'rf',
                   tuneGrid=tuneGrid, 
                   trControl = control)
print(rf_random_grid)

```



```{r}
set.seed(10)
pred.m2=predict(rf_random, newdata=Bhousing.testing)
actuals_predicts= data.frame(cbind(actuals=Bhousing.testing$MEDV, predicts=exp(pred.m2)))

head(actuals_predicts)

```

```{r}
library(DMwR)
library(lattice)
library(grid)
set.seed(10)
r=regr.eval(actuals_predicts$actuals,actuals_predicts$predicts)
r
```

We see that there isn't much change in the RMSE value after doing a grid search, the mtry value is still 5. So we shall stick to __Random Forest__ as our best model.
